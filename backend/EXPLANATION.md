# GradeSense Backend — How It All Works

This document explains the entire backend codebase: what each piece does, how they connect, and the thinking behind the design decisions.

---

## What Is GradeSense?

GradeSense is an AI-powered exam grading tool. Teachers upload question papers, model answers, and student answer sheets (handwritten PDFs/images). The system uses Google Gemini (vision + text) to:

1. Extract questions from a question paper PDF
2. Extract model answers from a model answer sheet via OCR
3. Grade each student's handwritten answers against the model answers
4. Generate annotated images with feedback marks (ticks, crosses, comments) overlaid on the student's paper
5. Provide analytics, reports, and re-evaluation workflows

The frontend is React. The backend is Python (FastAPI). The database is MongoDB. AI is Google Gemini 2.5 Flash.

---

## The Two Backends: `server.py` vs `app/`

This is the most important thing to understand.

### `server.py` (~14,000 lines) — The Monolith (Currently Running)

This is the actual production server. It contains *everything* in a single file:

- All FastAPI route definitions (100+ endpoints)
- All Pydantic models (User, Exam, Submission, etc.)
- All business logic (grading, extraction, analytics)
- All AI prompt engineering (grading modes, UPSC-specific prompts)
- Authentication (Google OAuth + email/password)
- Background job processing
- Image annotation generation
- Caching logic
- Admin panel endpoints
- Student dashboard endpoints
- Natural language "Ask Your Data" analytics
- Notification system
- GridFS file storage for large documents

It's started via `start_backend.sh` which runs `uvicorn server:app`.

### `app/` — The Modular Rewrite (Partially Complete)

This is an attempt to break the monolith into clean modules. It has:

- `app/config/settings.py` — centralized configuration
- `app/cache/__init__.py` — 3-level caching (questions, model answers, grading results)
- `app/services/` — core business logic as independent service classes
- `app/routes/` — API endpoint definitions
- `app/models/__init__.py` — Pydantic models for the new architecture
- `app/utils/` — shared utilities
- `app/workers/` — background task processing

It's started via `main.py` which uses the `app/` package. However, this rewrite only covers the core grading pipeline (upload question paper → extract → upload model answer → extract → grade). It does NOT cover the 100+ other endpoints in `server.py` (auth, batches, students, analytics, admin, notifications, etc.).

**Bottom line: `server.py` is what runs in production. `app/` is the future architecture, partially built.**

---

## The Grading Pipeline (Core Flow)

This is the heart of the system. Here's how a paper gets graded, step by step:

### Phase 1: Exam Setup

1. Teacher creates an exam (name, batch, subject, total marks, grading mode)
2. Teacher uploads a question paper PDF
3. The PDF is converted to base64 JPEG images (via PyMuPDF at 2x zoom)
4. Images are sent to Gemini with a prompt asking it to extract all questions as structured JSON
5. Extracted questions (number, text, max marks, rubric, sub-questions) are stored in the exam document
6. Results are cached by `exam_id + pdf_hash` so re-uploads of the same PDF skip AI calls

### Phase 2: Model Answer (Optional)

1. Teacher uploads a model answer PDF
2. PDF → images (same as above)
3. For each question, Gemini Vision OCR extracts the answer text from the model answer sheet
4. Each answer is cached by `exam_id + question_number + pdf_hash`
5. The full model answer text is also extracted as a single blob for text-based grading

### Phase 3: Student Paper Grading

1. Teacher uploads student papers (PDFs, images, or a ZIP)
2. Each PDF is converted to images
3. Student info (name, roll number) is extracted from the first page using Gemini
4. For each question in the exam:
   - Student answer images are sent to Gemini along with the question text and model answer
   - Gemini returns: marks, feedback, confidence score, status, sub-scores, and annotation data
5. Results are compiled into a submission document
6. Annotated images are generated by overlaying marks/comments on the student's paper

### Chunking

Student papers can be 50+ pages. Gemini has context limits. So papers are split into chunks of ~10 pages with 1-page overlap. Each chunk is graded independently, then results are merged. Questions marked as "not found" (-1.0) in one chunk may be found in another.

### Caching Strategy

Three levels of caching prevent redundant AI calls:

| Level | What's Cached | Key | TTL |
|-------|--------------|-----|-----|
| Questions | Extracted question structure | `exam_id + pdf_hash` | 30 days |
| Model Answers | OCR'd answer per question | `exam_id + question_number + pdf_hash` | 30 days |
| Grading Results | Score for a student's answer | `exam_id + student_hash + question_number` | 30 days |

In-memory caching (Python dict) is also used in `server.py` for grading results during a session.

---

## Grading Modes

The system supports four grading modes, each with different strictness levels:

| Mode | Philosophy | Partial Credit | Threshold |
|------|-----------|----------------|-----------|
| Strict | UPSC-level. All or nothing per question/sub-question. | None. Perfect = full marks, any error = 0. | 70% |
| Balanced | Fair academic evaluation. Process + outcome both matter. | Yes. 60% weight on method, 40% on answer. | 50% |
| Conceptual | Understanding over execution. | Yes. Alternative methods accepted. Minor errors forgiven. | 50% |
| Lenient | Effort-based. Reward attempts. | Yes. Floor marks for any genuine attempt. | 25% |

Strict mode also has UPSC-specific caps: maximum awardable marks = `(0.5 × max_marks) - 1`.

---

## UPSC vs College Grading

The system detects whether an exam is UPSC-style (civil services) or college-style based on exam name, subject name, and an explicit `exam_type` field. This changes the entire system prompt:

- **UPSC mode**: Uses a senior UPSC examiner persona. Checks for constitutional articles, committee references, data citations, diagram quality. Has paper-specific checks (GS-1 through GS-4, Ethics).
- **College mode**: Standard university evaluation. Definitions, examples, method marks for numericals.

---

## Background Job Processing

Grading 30+ papers synchronously would timeout HTTP requests. So grading runs as background jobs:

1. `grade_papers_background` creates a job document in MongoDB with status "processing"
2. `process_grading_job_in_background` is launched via `asyncio.create_task`
3. Each paper is graded sequentially (with retry + exponential backoff on failures)
4. Job progress is updated in MongoDB after each paper
5. Frontend polls `GET /api/grading-job/{job_id}` every 2 seconds for progress
6. On completion, a notification is created for the teacher

There's also a `task_worker.py` that implements a MongoDB-based task queue for more robust background processing, and `background_grading.py` which has the retry logic with exponential backoff.

---

## Image Annotations

After grading, the system generates annotated versions of student papers with visual feedback:

1. Gemini returns annotation data (anchor text, type, sentiment, page number)
2. `vision_ocr_service.py` uses Google Cloud Vision API to find the exact pixel coordinates of anchor text on the page
3. `annotation_utils.py` draws the annotations using Pillow:
   - Checkmarks (green) for correct answers
   - Cross marks (red) for errors
   - Underlines for key phrases
   - Box comments with feedback text
   - Margin notes with connector lines
   - Score boxes
   - Bracket annotations for grouped feedback

The annotation system supports multiple styles: `TICK`, `CROSS`, `BOX_COMMENT`, `MARGIN_LEASH`, `MARGIN_NOTE`, `EMPHASIS_UNDERLINE`, `DOUBLE_TICK`, `FEEDBACK_UNDERLINE`, `GROUP_BRACKET`.

---

## Authentication

Two auth methods:

1. **Google OAuth** — Primary. Uses Emergent-managed OAuth flow. Callback at `/auth/google/callback`. Creates session tokens stored as httpOnly cookies.
2. **Email/Password** — Fallback. Uses bcrypt hashing (`auth_utils.py`) and JWT tokens (`python-jose`). 7-day token expiry.

Session tokens are stored in a `user_sessions` MongoDB collection. The `get_current_user` dependency extracts the user from the session cookie on every authenticated request.

---

## File Handling

Large files (PDFs, images) are stored in MongoDB GridFS to avoid the 16MB BSON document size limit. `file_utils.py` handles:

- PDF → image conversion (PyMuPDF)
- Word → PDF conversion (via LibreOffice subprocess)
- ZIP extraction (for bulk student paper uploads)
- Student name/ID parsing from filenames
- Google Drive file downloads

Images are also rotation-corrected before grading using Pillow-based orientation detection.

---

## Database Collections

MongoDB collections used (all in a single database):

| Collection | Purpose |
|-----------|---------|
| `users` | User accounts (teachers + students) |
| `user_sessions` | Active login sessions |
| `batches` | Class/section groupings |
| `subjects` | Subject definitions per teacher |
| `exams` | Exam metadata, questions, status |
| `submissions` | Graded student papers with scores |
| `model_answers` | Extracted model answer text per question |
| `grading_jobs` | Background grading job tracking |
| `questions_cache` | Cached extracted questions |
| `model_answer_cache` | Cached model answer OCR results |
| `grading_result_cache` | Cached grading results |
| `grading_results` | Persistent grading result cache |
| `notifications` | User notifications |
| `re_evaluations` | Student re-evaluation requests |
| `grading_feedback` | Teacher corrections to AI grading |
| `user_feedback` | General user feedback (bugs, suggestions) |
| `api_metrics` | API call tracking |
| `user_events` | Frontend event tracking |
| `grading_analytics` | Detailed grading performance data |
| `study_materials` | Study resources per subject |
| `fs.files` / `fs.chunks` | GridFS file storage |

---

## Analytics & Reporting

The system provides extensive analytics:

- **Class Reports**: Score distributions, pass rates, top/bottom performers
- **Topic Mastery Heatmap**: Per-topic mastery levels extracted from rubrics
- **Class Insights**: AI-generated insights about class performance patterns
- **Misconception Analysis**: Common errors identified across submissions
- **Student Deep Dive**: Individual student performance breakdown
- **Student Journey**: Performance trends over time
- **Bluff Index**: Detects students who may be gaming the system
- **Syllabus Coverage**: How much of the syllabus has been assessed
- **Peer Group Suggestions**: Groups students by performance for targeted teaching
- **Ask Your Data**: Natural language queries powered by Gemini ("Show me top 5 students in Math")
- **Review Packet Generation**: AI-generated practice questions based on weak areas

---

## Teacher Learning Patterns

When a teacher corrects an AI grade (via the "Improve AI Grading" modal), the correction is stored as a `grading_feedback` document. On subsequent gradings, the system fetches the teacher's past corrections for the same subject and includes them in the Gemini prompt as "learned patterns." This makes the AI adapt to the teacher's grading style over time.

---

## The `models/` Directory

Two separate model directories exist:

1. **`backend/models/`** — Pydantic models used by `server.py`. These define the request/response shapes for all API endpoints. They're imported directly by the monolith.

2. **`backend/app/models/`** — Pydantic models for the `app/` rewrite. Simpler, covering only the core grading pipeline entities.

---

## The `scripts/` Directory

Migration scripts for moving data to GridFS:

- `migrate_submissions_to_gridfs.py` — Moves submission file data from inline BSON to GridFS
- `migrate_submission_images_to_gridfs.py` — Moves base64 images to GridFS
- `migrate_large_files_to_gridfs.py` — General large file migration

These exist because the original design stored images directly in MongoDB documents, which hit the 16MB BSON limit when grading papers with many pages.

---

## External Dependencies

Key Python packages:

| Package | Purpose |
|---------|---------|
| `fastapi` | Web framework |
| `uvicorn` | ASGI server |
| `motor` | Async MongoDB driver |
| `pymongo` | Sync MongoDB driver (for GridFS) |
| `google-generativeai` | Gemini AI SDK |
| `PyMuPDF` (`fitz`) | PDF → image conversion |
| `Pillow` | Image processing and annotation drawing |
| `python-jose` | JWT token handling |
| `passlib` | Password hashing (bcrypt) |
| `httpx` | HTTP client (for OAuth callbacks) |
| `pydantic` | Data validation and serialization |

---

## Startup Flow

### Production (`start_backend.sh`)
1. Runs `check_dependencies.sh` (ensures poppler-utils is installed)
2. Starts `uvicorn server:app` on port 8001

### Development (`start_backend_v2.sh`)
1. Creates/activates Python venv
2. Installs requirements
3. Checks for `.env` file
4. Starts `uvicorn main:app --reload` on port 8001

### What happens on startup (`server.py`)
1. Loads `.env` file
2. Connects to MongoDB (async via Motor + sync via PyMongo for GridFS)
3. Configures Gemini API key
4. Starts a background worker loop that cleans up old metrics every 24 hours
5. Registers all routes and middleware
6. Creates database indexes

---

## Key Design Decisions & Trade-offs

1. **Monolith over microservices**: Everything in one process for simplicity. The `app/` rewrite modularizes code but keeps it in one deployable unit.

2. **Gemini for everything**: Question extraction, answer OCR, grading, analytics insights, annotation generation — all use the same Gemini 2.5 Flash model. This simplifies the stack but creates a single point of failure.

3. **GridFS for large files**: MongoDB's 16MB BSON limit forced a migration to GridFS for storing PDFs and images. This adds complexity but avoids needing a separate file storage service.

4. **Chunked grading**: Papers are split into 10-page chunks to stay within Gemini's context window. Overlap pages prevent questions from being split across chunks.

5. **In-memory + database caching**: Fast in-memory cache for the current session, persistent MongoDB cache for cross-session reuse. TTL-based expiry keeps the cache fresh.

6. **Background jobs via asyncio**: Simple `asyncio.create_task` for background grading instead of a full task queue (Celery, etc.). The `task_worker.py` adds a MongoDB-based queue for more robustness.

7. **UPSC-specific grading**: The system has deep UPSC exam knowledge baked into the prompts. This is a domain-specific feature, not a generic grading system.
